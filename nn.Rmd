# Neural Net

The following example follows comes from [Andrew Trask's old blog ost](https://iamtrask.github.io/2015/07/12/basic-python-network/), which is nice because it tries to demonstrate a neural net via backpropagation in very few lines, much like this document does elsewhere.

- https://iamtrask.github.io/2015/07/12/basic-python-network/
- https://iamtrask.github.io/2015/07/27/python-network-part2/


The data setup is very simple, and I keep the Python code almost identical outside of very slight cosmetic (mostly name/space) changes.  For more detail I suggest following the posts, but I'll add some here and there.



## Example 1
### Python

From the blog:

> 
- **X** 	Input dataset matrix where each row is a training example
- **y** 	Output dataset matrix where each row is a training example
- **layer_0** 	First Layer of the Network, specified by the input data
- **layer_1** 	Second Layer of the Network, otherwise known as the hidden layer
- **synapse_0** 	First layer of weights, Synapse 0, connecting layer_0 to layer_1.
- **\*** 	Elementwise multiplication, so two vectors of equal size are multiplying corresponding values 1-to-1 to generate a final vector of identical size.
- **-** 	Elementwise subtraction, so two vectors of equal size are subtracting corresponding values 1-to-1 to generate a final vector of identical size.
- **x.dot(y)** 	If x and y are vectors, this is a dot product. If both are matrices, it's a matrix-matrix multiplication. If only one is a matrix, then it's vector matrix multiplication.


In this initial example, while it can serve as instructive starting point for backpropagation, we're not really using a neural net, but rather just an alternative way to estimate a logistic regression. `layer_1` in this case is just the linear predictor after transformation.

Note that in this particular example however, that the first column is perfectly correlated with the target `y`, which would cause a problem if no regularization were applied.

```{python py_demo1}
import numpy as np

# sigmoid function
def nonlin(x, deriv = False):
    if(deriv == True):
        return x*(1 - x)
    return 1/(1 + np.exp(-x))
    
# input dataset
X = np.array([ [0, 0, 1],
               [0, 1, 1],
               [1, 0, 1],
               [1, 1, 1] ])
    
# output dataset            
y = np.array([[0, 0, 1, 1]]).T

# seed random numbers to make calculation
# deterministic (just a good practice)
np.random.seed(1)

# initialize weights randomly with mean 0 (or just use np.random.uniform)
synapse_0 = 2*np.random.random((3, 1)) - 1

for iter in np.arange(10000):

    # forward propagation
    layer_0 = X
    layer_1 = nonlin(np.dot(layer_0, synapse_0))

    # how much did we miss?
    l1_error = y - layer_1

    # multiply how much we missed by the 
    # slope of the sigmoid at the values in layer_1
    l1_delta = l1_error * nonlin(layer_1,True)

    # update weights
    synapse_0 += np.dot(layer_0.T, l1_delta)

print("Output After Training:")
print(np.append(layer_1, y, axis = 1))
```


### R

For R I make a couple changes, but it should be easy to follow.  The key idea here is the update step. Using the derivative, we are getting the slope of the sigmoid function at the point of interest.  

```{r nn-sigmoid, echo=FALSE}
library(tidyverse)
d = tibble(x = runif(1000, -4, 4), y = plogis(x))
points = c(-1, 0, 2)
points_prob = plogis(points)

tangent_ints = points_prob - deriv*points
deriv = points_prob * (1 - points_prob)

x_minus = points - .25
x_plus  = points + .25

gdata = pmap_df(
  list(x_minus, x_plus, tangent_ints, deriv),
  .f = function(x1, x2, s, t)
    data.frame(
      x1 = x1,
      x2 = x2,
      y1 = c(1, x1) %*% c(s, t),
      y2 = c(1, x2) %*% c(s, t)
    )
)

gdata = gdata %>% 
  mutate(point = factor(points))

d %>%
  ggplot(aes(x, y)) +
  geom_line(alpha = .25) +
  geom_point(
    aes(color = factor(x)),
    data = data.frame(x = points, y = points_prob),
    show.legend = FASLE
  ) +
  geom_segment(
    aes(
      x     = x1,
      xend  = x2,
      y     = y1,
      yend  = y2,
      color = point
    ),
    size = 1,
    data = gdata,
    show.legend = FALSE
  ) +
  scico::scale_color_scico_d(begin = .25, end = .75)
```


If a probability (`l1`) is close to zero or 1, this suggests the prediction 'confidence' is high, and the result is that there is less need for updating the weight.  For the others, e.g. close to `x = 0`, the predictions will be relatively more updated, as the error will be greater.

```{r r_demo1}
X = matrix( 
  c(0, 0, 1, 
    0, 1, 1, 
    1, 0, 1, 
    1, 1, 1),
  nrow  = 4,
  ncol  = 3,
  byrow = TRUE
)

# output dataset            
y = c(0, 0, 1, 1)

# seed random numbers to make calculation
# deterministic (just a good practice)
set.seed(1)

# initialize weights randomly with mean 0
synapse_0 = matrix(runif(3, min = -1, max = 1), 3, 1)
# synapse_0 = py$synapse_0

# sigmoid function
nonlin <- function(x, deriv = FALSE) {
  if (deriv)
    x * (1 - x)
  else
    plogis(x)
}


for (iter in 1:10000) {

    # forward propagation
    layer_0 = X
    layer_1 = nonlin(layer_0 %*% synapse_0)

    # how much did we miss?
    l1_error = y - layer_1

    # multiply how much we missed by the 
    # slope of the sigmoid at the values in layer_1
    l1_delta = l1_error * nonlin(layer_1, deriv = TRUE)

    # update weights
    synapse_0 = synapse_0 + crossprod(layer_0, l1_delta)
}

message("Output After Training: \n", 
        paste0(capture.output(cbind(layer_1, y)), collapse = '\n'))

```



## Example 2

### Python

```{python py_demo2,  eval = T}
import numpy as np

X = np.array([ [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1] ])
y = np.array([[0, 1, 1, 0]]).T

np.random.seed(1)

syn0_init = 2*np.random.random((3, 4)) - 1
syn1_init = 2*np.random.random((4, 1)) - 1

synapse_0 = syn0_init
synapse_1 = syn1_init

# for testing
# synapse_0 = np.array([ [.1,.1,.1], [0, 0, 0], [0, 0, 0], [-.1, -.1, -.1] ]).T
# synapse_1 = np.array([[.5, .5, -.5, -.5]]).T

for j in range(60000):
    layer_1 = 1 / (1 + np.exp(-(np.dot(X, synapse_0))))
    l2 = 1 / (1 + np.exp(-(np.dot(layer_1, synapse_1))))
    
    l2_delta = (y - l2) * (l2 * (1-l2))
    l1_delta = l2_delta.dot(synapse_1.T) * (layer_1 * (1 - layer_1))
    
    synapse_1 += layer_1.T.dot(l2_delta)
    synapse_0 +=  X.T.dot(l1_delta)
    
layer_1.round(3)
np.append(l2.round(3), y, axis = 1)
l2_delta.round(3)
l1_delta.round(3)
synapse_1.round(3)
synapse_0.round(3)
```

### R



```{r r_demo2}
X = matrix(
  c(0, 0, 1,
    0, 1, 1,
    1, 0, 1,
    1, 1, 1),
  nrow = 4,
  ncol = 3,
  byrow = TRUE
)

y = matrix(c(0, 1, 1, 0), ncol = 1)

set.seed(1)

# create the same starting point to have results as similar as possible.
# synapse_0 = py$syn0_init
# synapse_1 = py$syn1_init

# or do randomly in same fashion
synapse_0 = matrix(2 * runif(12) - 1, 3, 4)
synapse_1 = matrix(2 * runif(12) - 1, 4, 1)

synapse_0
synapse_1


for (j in 1:60000) {
  layer_1 = plogis(X  %*%  synapse_0)         # 4 x 4
  l2 = plogis(layer_1 %*%  synapse_1)         # 4 x 1
  l2_delta = (y - l2) * (l2 * (1 - l2))
  l1_delta = tcrossprod(l2_delta, synapse_1) * (layer_1 * (1 - layer_1))
  synapse_1 = synapse_1 + crossprod(layer_1, l2_delta)
  synapse_0 = synapse_0 + crossprod(X,  l1_delta)
}

round(layer_1, 3)
round(cbind(l2, y), 3)
round(l2_delta, 3)
round(l1_delta, 3)
round(synapse_1, 3)
round(synapse_0, 3)
```

## Example 3

Two layer NN



```{python 2layernn-py, eval = F}
# sigmoid function
def nonlin(x, deriv = False):
    if(deriv == True):
        return x * (1 - x)
    return 1/(1 + np.exp(-x))
    
# input dataset
X = np.array([  [0, 0, 1], 
                [0, 1, 1], 
                [1, 0, 1], 
                [1, 1, 1] ])
    
# output dataset            
y = np.array([[0, 0, 1, 1]]).T

# seed random numbers to make calculation
# deterministic (just a good practice)
np.random.seed(1)

# initialize weights randomly with mean 0
syn0_init = 2*np.random.random((3, 1)) - 1
synapse_0 = syn0_init

for iter in range(10000):

    # forward propagation
    layer_0 = X
    layer_1 = nonlin(np.dot(layer_0, synapse_0))

    # how much did we miss?
    l1_error = y - layer_1

    # multiply how much we missed by the 
    # slope of the sigmoid at the values in layer_1
    l1_delta = l1_error * nonlin(layer_1, True)

    # update weights
    synapse_0 += np.dot(layer_0.T, l1_delta)

"Output After Training:"
layer_1
l1_delta

```


```{r 2layernn-r, eval=FALSE}
# input dataset
X = matrix(
  c(0, 0, 1,
    0, 1, 1,
    1, 0, 1,
    1, 1, 1),
  nrow = 4,
  ncol = 3,
  byrow = T
)
    
# output dataset            
y = matrix(c(0, 0, 1, 1), ncol = 1)

nonlin = function(x, deriv = FALSE){
  if(deriv == TRUE)
    x * (1-x)
  
  plogis(x)
}
    
synapse_0 = reticulate::py$syn0_init

for (i in 1:10000) {

    # forward propagation
    layer_0 = X
    layer_1 = nonlin(layer_0 %*%  synapse_0)

    # how much did we miss?
    l1_error = y - layer_1

    # multiply how much we missed by the 
    # slope of the sigmoid at the values in layer_1
    l1_delta = l1_error * nonlin(layer_1, TRUE)

    # update weights
    synapse_0 = synapse_0 + crossprod(layer_0, l1_delta)
}

message("Output After Training:\nn")
layer_1

l1_delta
```

