# Stochastic Gradient Descent

Here we have 'online' learning via stochastic gradient descent.  See the
[standard gradient descent][Gradient Descent] chapter. In the following, we have
basic data for standard regression, but in this 'online' learning case, we can
assume each observation comes to us as a stream over time rather than as a
single batch, and would continue coming in.  Note that there are plenty of
variations of this, and it can be applied in the batch case as well.  Currently
no stopping point is implemented in order to trace results over all data
points/iterations. On revisiting this much later, I thought it useful to add
that I believe this was motivated by the example in Murphy's Probabilistic
Machine Learning text.

## Data Setup

Create some data for a standard linear regression.

```{r sgd-setup}
library(tidyverse)

set.seed(1234)

n  = 1000
x1 = rnorm(n)
x2 = rnorm(n)
y  = 1 + .5*x1 + .2*x2 + rnorm(n)
X  = cbind(Intercept = 1, x1, x2)
```




## Function

The estimating function.

```{r sgd}
sgd <- function(
  par,                                      # parameter estimates
  X,                                        # model matrix
  y,                                        # target variable
  stepsize = 1,                             # the learning rate
  stepsizeTau = 0,                          # if > 0, a check on the LR at early iterations
  average = FALSE
){
  
  # initialize
  beta = par
  names(beta) = colnames(X)
  betamat = matrix(0, nrow(X), ncol = length(beta))      # Collect all estimates
  fits = NA                                              # fitted values
  s = 0                                                  # adagrad per parameter learning rate adjustment
  loss = NA                                              # Collect loss at each point
  
  for (i in 1:nrow(X)) {
    Xi   = X[i, , drop = FALSE]
    yi   = y[i]
    LP   = Xi %*% beta                                   # matrix operations not necessary, 
    grad = t(Xi) %*% (LP - yi)                           # but makes consistent with the  standard gd
    s    = s + grad^2
    beta = beta - stepsize * grad/(stepsizeTau + sqrt(s))     # adagrad approach
    
    if (average & i > 1) {
      beta =  beta - 1/i * (betamat[i - 1, ] - beta)          # a variation
    } 
    
    betamat[i,] = beta
    fits[i]     = LP
    loss[i]     = (LP - yi)^2
  }
  
  LP = X %*% beta
  lastloss = crossprod(LP - y)
  
  list(
    par    = beta,                                       # final estimates
    parvec = betamat,                                    # all estimates
    loss   = loss,                                       # observation level loss
    RMSE   = sqrt(sum(lastloss)/nrow(X)),
    fitted = fits
  )
}
```




## Estimation


Set starting values.

```{r sgd-start}
init = rep(0, 3)
```

For any particular data you might have to fiddle with the `stepsize`, perhaps
choosing one based on cross-validation with old data.

```{r sgd-est}
fit_sgd = sgd(
  init,
  X = X,
  y = y,
  stepsize    = .1,
  stepsizeTau = .5,
  average = FALSE
)

str(fit_sgd)

fit_sgd$par
```



## Comparison

We can compare to standard linear regression.

```{r sgd-compare-lm}
# summary(lm(y ~ x1 + x2))
coef1 = coef(lm(y ~ x1 + x2))
```

```{r sgd-compare-lm-show, echo=FALSE}
rbind(
  fit_sgd = fit_sgd$par[, 1],
  lm = coef1
) %>% 
  kable_df()
```




## Visualize Estimates


```{r sgd-visualize, echo=FALSE}
library(tidyverse)

gd = data.frame(fit_sgd$parvec) %>% 
  mutate(Iteration = 1:n())

gd = gd %>%
  pivot_longer(cols = -Iteration,
               names_to = 'Parameter',
               values_to = 'Value') %>%
  mutate(Parameter = factor(Parameter, labels = colnames(X)))

ggplot(aes(
  x = Iteration,
  y = Value,
  group = Parameter,
  color = Parameter
),
data = gd) +
  geom_path() +
  geom_point(data = filter(gd, Iteration == n), size = 3) +
  geom_text(
    aes(label = round(Value, 2)),
    hjust = -.5,
    angle = 45,
    size  = 4,
    data  = filter(gd, Iteration == n)
  ) + 
  scico::scale_color_scico_d(end = .75)
```





## Data Set Shift

This data includes a shift of the previous data, where the data fundamentally changes at certain times.

```{r sgd-shift-setup}
set.seed(1234)

n2   = 1000
x1.2 = rnorm(n2)
x2.2 = rnorm(n2)
y2 = -1 + .25*x1.2 - .25*x2.2 + rnorm(n2)
X2 = rbind(X, cbind(1, x1.2, x2.2))
coef2 = coef(lm(y2 ~ x1.2 + x2.2))
y2 = c(y, y2)

n3    = 1000
x1.3  = rnorm(n3)
x2.3  = rnorm(n3)
y3    = 1 - .25*x1.3 + .25*x2.3 + rnorm(n3)
coef3 = coef(lm(y3 ~ x1.3 + x2.3))

X3 = rbind(X2, cbind(1, x1.3, x2.3))
y3 = c(y2, y3)
```





### Estimation

We'll use the same function as before.

```{r sgd-est-2}
fit_sgd2 = sgd(
  init,
  X = X3,
  y = y3,
  stepsize    = 1,
  stepsizeTau = 0,
  average = FALSE
)

str(fit_sgd2)
```

### Comparison

Compare with <span class="func" style = "">lm</span> result for each data part.

```{r sgd-compare-2, echo=F}
lm_coef = rbind(lm_part1 = coef1, lm_part2 = coef2, lm_part3 = coef3) %>% 
  data.frame() %>% 
  rename(Intercept = X.Intercept.)

sgd_coef = fit_sgd2$parvec[c(n, n + n2, n + n2 + n3), ] %>% 
  data.frame()

rownames(sgd_coef) = c('sgd_part1','sgd_part2','sgd_part3')
colnames(sgd_coef) = colnames(lm_coef)

bind_rows(lm_coef, sgd_coef) %>% 
  kable_df()
```


### Visualize Estimates

Visualize estimates across iterations.

```{r sgd-visualize-2, echo=FALSE}
gd = data.frame(fit_sgd2$parvec) %>% 
  mutate(Iteration = 1:n())

gd = gd %>% 
  pivot_longer(cols = -Iteration,
               names_to = 'Parameter', 
               values_to = 'Value') %>% 
  mutate(Parameter = factor(Parameter, labels = colnames(X)))


ggplot(aes(x = Iteration,
           y = Value,
           group = Parameter,
           color = Parameter
           ),
       data = gd) +
  geom_path() +
  geom_point(data = filter(gd, Iteration %in% c(n, n + n2, n + n2 + n3)),
             size = 3) +
  geom_text(
    aes(label = round(Value, 2)),
    hjust = -.5,
    angle = 45,
    data = filter(gd, Iteration %in% c(n, n + n2, n + n2 + n3)),
    size = 4,
    show.legend = FALSE
  ) + 
  scico::scale_color_scico_d(end = .75, alpha = .5)
```



## Source

Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/stochastic_gradient_descent.R
